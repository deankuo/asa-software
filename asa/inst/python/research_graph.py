# research_graph.py
#
# LangGraph multi-agent orchestration for open-ended research tasks.
# Simplified architecture: Planner -> Searcher -> Extractor -> Stopper
#
import hashlib
import json
import logging
import time
from dataclasses import dataclass, field
from typing import Any, Annotated, Dict, List, Optional, Sequence, TypedDict

from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
)
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode

from state_utils import add_to_list, merge_dicts, parse_llm_json

logger = logging.getLogger(__name__)


# ────────────────────────────────────────────────────────────────────────
# Configuration
# ────────────────────────────────────────────────────────────────────────
@dataclass
class ResearchConfig:
    """Configuration for research task execution."""
    max_workers: int = 4
    max_rounds: int = 8
    budget_queries: int = 50
    budget_tokens: int = 200000
    budget_time_sec: int = 300
    target_items: Optional[int] = None
    plateau_rounds: int = 2
    novelty_min: float = 0.05
    novelty_window: int = 20
    use_wikidata: bool = True
    use_web: bool = True
    use_wikipedia: bool = True
    # Temporal filtering parameters
    time_filter: Optional[str] = None      # DDG time filter: "d", "w", "m", "y"
    date_after: Optional[str] = None       # ISO 8601: "2020-01-01"
    date_before: Optional[str] = None      # ISO 8601: "2024-01-01"
    temporal_strictness: str = "best_effort"  # "best_effort" | "strict"
    use_wayback: bool = False              # Use Wayback Machine for pre-date guarantees


# ────────────────────────────────────────────────────────────────────────
# State Definitions
# ────────────────────────────────────────────────────────────────────────
class ResultRow(TypedDict):
    """A single result row with provenance."""
    row_id: str
    fields: Dict[str, Any]
    source_url: str
    confidence: float
    worker_id: str
    extraction_timestamp: float


class ResearchState(TypedDict):
    """Main state for research graph."""
    # Input
    query: str
    schema: Dict[str, str]
    config: Dict[str, Any]

    # Planning
    plan: Dict[str, Any]
    entity_type: str
    wikidata_type: Optional[str]

    # Results
    results: Annotated[List[ResultRow], add_to_list]
    seen_hashes: Annotated[Dict[str, bool], merge_dicts]

    # Metrics & Control
    round_number: int
    queries_used: int
    status: str  # "planning", "searching", "complete", "failed"
    stop_reason: Optional[str]
    errors: Annotated[List[Dict], add_to_list]


# ────────────────────────────────────────────────────────────────────────
# Utility Functions
# ────────────────────────────────────────────────────────────────────────
def _hash_result(fields: Dict[str, Any], schema: Dict[str, str]) -> str:
    """Create a hash for deduplication based on key fields."""
    key_fields = list(schema.keys())[:3]
    key_values = []
    for field_name in key_fields:
        val = fields.get(field_name, "")
        if isinstance(val, str):
            val = val.lower().strip()
        key_values.append(str(val))
    key_string = "|".join(key_values)
    return hashlib.md5(key_string.encode()).hexdigest()


def _fuzzy_match_name(name1: str, name2: str) -> float:
    """Simple fuzzy matching for names. Returns similarity score 0-1."""
    if not name1 or not name2:
        return 0.0
    name1 = name1.lower().strip()
    name2 = name2.lower().strip()
    if name1 == name2:
        return 1.0
    if name1 in name2 or name2 in name1:
        return 0.8
    words1 = set(name1.split())
    words2 = set(name2.split())
    if not words1 or not words2:
        return 0.0
    overlap = len(words1 & words2)
    total = len(words1 | words2)
    return overlap / total if total > 0 else 0.0


# ────────────────────────────────────────────────────────────────────────
# Planner Node
# ────────────────────────────────────────────────────────────────────────
PLANNER_PROMPT = """You are a research planner. Analyze the query and identify:
1. The entity type being requested
2. Whether this matches a known Wikidata entity type

Known Wikidata entity types:
- us_senators: Current United States Senators
- us_representatives: Current US Representatives
- countries: All countries in the world
- fortune500: Fortune 500 companies
- us_states: US States and territories

Query: {query}

Respond in JSON format ONLY (no markdown):
{{"entity_type": "description of entity type", "wikidata_type": "us_senators|us_representatives|countries|fortune500|us_states|null", "search_queries": ["query1", "query2"]}}
"""


def create_planner_node(llm):
    """Create the planner node."""

    def planner_node(state: ResearchState) -> Dict:
        """Analyze query and create research plan."""
        query = state.get("query", "")
        logger.info(f"Planner analyzing: {query}")

        prompt = PLANNER_PROMPT.format(query=query)
        messages = [HumanMessage(content=prompt)]

        try:
            response = llm.invoke(messages)
            plan = parse_llm_json(response.content)
            logger.info(f"Plan: entity={plan.get('entity_type')}, wikidata={plan.get('wikidata_type')}")

            return {
                "plan": plan,
                "entity_type": plan.get("entity_type", "unknown"),
                "wikidata_type": plan.get("wikidata_type"),
                "status": "searching",
                "queries_used": 1
            }

        except Exception as e:
            logger.error(f"Planner error: {e}")
            return {
                "status": "failed",
                "stop_reason": f"Planning failed: {str(e)}",
                "errors": [{"stage": "planner", "error": str(e)}]
            }

    return planner_node


# ────────────────────────────────────────────────────────────────────────
# Searcher Node
# ────────────────────────────────────────────────────────────────────────
def create_searcher_node(llm, tools, wikidata_tool=None, research_config: ResearchConfig = None):
    """Create the searcher node that executes searches."""

    def searcher_node(state: ResearchState) -> Dict:
        """Execute search based on plan."""
        wikidata_type = state.get("wikidata_type")
        query = state.get("query", "")
        schema = state.get("schema", {})
        config = state.get("config", {})

        # Extract temporal config
        time_filter = config.get("time_filter") or (research_config.time_filter if research_config else None)
        date_after = config.get("date_after") or (research_config.date_after if research_config else None)
        date_before = config.get("date_before") or (research_config.date_before if research_config else None)

        results = []
        queries_used = state.get("queries_used", 0)

        # Try Wikidata first if we have a matching type
        if wikidata_type and wikidata_tool and config.get("use_wikidata", True):
            logger.info(f"Querying Wikidata for: {wikidata_type}")
            try:
                wd_query = f"entity_type:{wikidata_type}"
                wd_result = wikidata_tool._run(wd_query)

                # Parse Wikidata results
                if isinstance(wd_result, str) and "Found" in wd_result:
                    lines = wd_result.split("\n")
                    for line in lines[1:]:  # Skip "Found X results:" line
                        if line.strip() and line[0].isdigit():
                            # Parse "1. Name (State) [Party] [Q123]"
                            parts = line.split(".", 1)
                            if len(parts) > 1:
                                info = parts[1].strip()
                                # Extract name (before first parenthesis)
                                name = info.split("(")[0].strip() if "(" in info else info.split("[")[0].strip()

                                # Extract other fields from parentheses/brackets
                                fields = {"name": name}

                                # Try to extract state
                                if "(" in info and ")" in info:
                                    state_match = info[info.find("(")+1:info.find(")")]
                                    fields["state"] = state_match

                                # Try to extract party
                                if "[" in info:
                                    bracket_content = info[info.find("[")+1:]
                                    if "]" in bracket_content:
                                        party = bracket_content[:bracket_content.find("]")]
                                        if not party.startswith("Q"):  # Skip Wikidata IDs
                                            fields["party"] = party

                                results.append(ResultRow(
                                    row_id=f"wd_{len(results)}",
                                    fields=fields,
                                    source_url="https://www.wikidata.org",
                                    confidence=0.95,
                                    worker_id="wikidata",
                                    extraction_timestamp=time.time()
                                ))

                queries_used += 1
                logger.info(f"Wikidata returned {len(results)} results")

            except Exception as e:
                logger.error(f"Wikidata error: {e}")

        # If no Wikidata results, try web search
        if len(results) == 0 and config.get("use_web", True):
            logger.info("Falling back to web search")
            try:
                # Apply temporal filter to search tools
                search_tools = []
                for tool in tools:
                    # Check if it's a DDG search tool and apply time filter
                    if hasattr(tool, 'api_wrapper') and time_filter:
                        tool.api_wrapper.time = time_filter
                        logger.info(f"Applied time filter '{time_filter}' to search tool")
                    search_tools.append(tool)

                # Use LLM with tools
                model_with_tools = llm.bind_tools(search_tools)
                tool_node = ToolNode(search_tools)

                search_prompt = f"""Search for: {query}
Extract entities with these fields: {list(schema.keys())}
Use the Search tool to find information."""

                messages = [HumanMessage(content=search_prompt)]

                for _ in range(3):  # Max 3 tool calls
                    response = model_with_tools.invoke(messages)
                    messages.append(response)
                    queries_used += 1

                    if hasattr(response, 'tool_calls') and response.tool_calls:
                        tool_result = tool_node.invoke({"messages": messages})
                        for msg in tool_result.get("messages", []):
                            messages.append(msg)
                    else:
                        break

            except Exception as e:
                logger.error(f"Web search error: {e}")

        return {
            "results": results,
            "queries_used": queries_used,
            "round_number": state.get("round_number", 0) + 1
        }

    return searcher_node


# ────────────────────────────────────────────────────────────────────────
# Deduplicator Node
# ────────────────────────────────────────────────────────────────────────
def create_deduper_node():
    """Create deduplication node."""

    def deduper_node(state: ResearchState) -> Dict:
        """Deduplicate results."""
        results = state.get("results", [])
        schema = state.get("schema", {})
        seen_hashes = state.get("seen_hashes", {})

        unique_results = []
        new_hashes = {}

        for result in results:
            fields = result.get("fields", {})
            result_hash = _hash_result(fields, schema)

            if result_hash not in seen_hashes and result_hash not in new_hashes:
                # Fuzzy check
                is_dup = False
                name = fields.get("name", "")
                if name:
                    for existing in unique_results:
                        if _fuzzy_match_name(name, existing.get("fields", {}).get("name", "")) > 0.85:
                            is_dup = True
                            break

                if not is_dup:
                    unique_results.append(result)
                    new_hashes[result_hash] = True

        logger.info(f"Dedup: {len(results)} -> {len(unique_results)}")

        return {
            "results": unique_results,
            "seen_hashes": new_hashes
        }

    return deduper_node


# ────────────────────────────────────────────────────────────────────────
# Stopper Node
# ────────────────────────────────────────────────────────────────────────
def create_stopper_node(config: ResearchConfig):
    """Create stopping criteria node."""

    def stopper_node(state: ResearchState) -> Dict:
        """Evaluate if we should stop."""
        results = state.get("results", [])
        round_num = state.get("round_number", 0)
        queries = state.get("queries_used", 0)

        # Check limits
        if queries >= config.budget_queries:
            return {"status": "complete", "stop_reason": "budget_queries"}

        if round_num >= config.max_rounds:
            return {"status": "complete", "stop_reason": "max_rounds"}

        if config.target_items and len(results) >= config.target_items:
            return {"status": "complete", "stop_reason": "target_reached"}

        # If we have results from Wikidata, we're done
        if len(results) > 0:
            return {"status": "complete", "stop_reason": "results_found"}

        return {"status": "searching"}

    return stopper_node


# ────────────────────────────────────────────────────────────────────────
# Graph Construction
# ────────────────────────────────────────────────────────────────────────
def should_continue(state: ResearchState) -> str:
    """Determine next step."""
    status = state.get("status", "")
    if status in ["complete", "failed"]:
        return "end"
    if status == "searching":
        return "search"
    return "end"


def create_research_graph(
    llm,
    tools: List,
    config: ResearchConfig = None,
    checkpointer=None,
    wikidata_tool=None
):
    """Create the research orchestration graph."""
    config = config or ResearchConfig()

    # Create nodes
    planner = create_planner_node(llm)
    searcher = create_searcher_node(llm, tools, wikidata_tool, research_config=config)
    deduper = create_deduper_node()
    stopper = create_stopper_node(config)

    # Build graph
    workflow = StateGraph(ResearchState)

    workflow.add_node("planner", planner)
    workflow.add_node("searcher", searcher)
    workflow.add_node("deduper", deduper)
    workflow.add_node("stopper", stopper)

    workflow.set_entry_point("planner")

    # Edges
    workflow.add_conditional_edges(
        "planner",
        lambda s: "end" if s.get("status") == "failed" else "search",
        {"search": "searcher", "end": END}
    )
    workflow.add_edge("searcher", "deduper")
    workflow.add_edge("deduper", "stopper")
    workflow.add_conditional_edges(
        "stopper",
        should_continue,
        {"search": "searcher", "end": END}
    )

    if checkpointer:
        return workflow.compile(checkpointer=checkpointer)
    return workflow.compile()


def run_research(
    graph,
    query: str,
    schema: Dict[str, str],
    config_dict: Dict[str, Any] = None,
    thread_id: str = None
) -> Dict[str, Any]:
    """Execute research graph and return results."""
    if thread_id is None:
        thread_id = hashlib.md5(f"{query}_{time.time()}".encode()).hexdigest()[:16]

    initial_state = {
        "query": query,
        "schema": schema,
        "config": config_dict or {},
        "plan": {},
        "entity_type": "",
        "wikidata_type": None,
        "results": [],
        "seen_hashes": {},
        "round_number": 0,
        "queries_used": 0,
        "status": "planning",
        "stop_reason": None,
        "errors": []
    }

    config = {"configurable": {"thread_id": thread_id}}
    start_time = time.time()

    try:
        final_state = graph.invoke(initial_state, config)
        elapsed = time.time() - start_time

        return {
            "results": final_state.get("results", []),
            "provenance": [],
            "metrics": {
                "round_number": final_state.get("round_number", 0),
                "queries_used": final_state.get("queries_used", 0),
                "time_elapsed": elapsed,
                "items_found": len(final_state.get("results", []))
            },
            "status": final_state.get("status", "unknown"),
            "stop_reason": final_state.get("stop_reason"),
            "errors": final_state.get("errors", []),
            "plan": final_state.get("plan", {})
        }

    except Exception as e:
        logger.error(f"Research error: {e}")
        return {
            "results": [],
            "provenance": [],
            "metrics": {"time_elapsed": time.time() - start_time},
            "status": "failed",
            "stop_reason": f"execution_error: {str(e)}",
            "errors": [{"stage": "execution", "error": str(e)}],
            "plan": {}
        }


def stream_research(
    graph,
    query: str,
    schema: Dict[str, str],
    config_dict: Dict[str, Any] = None,
    thread_id: str = None
):
    """Stream research progress and return final state.

    Yields progress events during execution, with the final 'complete' event
    containing the full result (same format as run_research).
    """
    if thread_id is None:
        thread_id = hashlib.md5(f"{query}_{time.time()}".encode()).hexdigest()[:16]

    initial_state = {
        "query": query,
        "schema": schema,
        "config": config_dict or {},
        "plan": {},
        "entity_type": "",
        "wikidata_type": None,
        "results": [],
        "seen_hashes": {},
        "round_number": 0,
        "queries_used": 0,
        "status": "planning",
        "stop_reason": None,
        "errors": []
    }

    config = {"configurable": {"thread_id": thread_id}}
    start_time = time.time()

    # Accumulate state updates (stream_mode="updates" gives partial updates per node)
    accumulated_state = dict(initial_state)

    try:
        for event in graph.stream(initial_state, config, stream_mode="updates"):
            node_name = list(event.keys())[0] if event else "unknown"
            node_state = event.get(node_name, {})

            # Merge node updates into accumulated state
            for key, value in node_state.items():
                accumulated_state[key] = value

            yield {
                "event_type": "node_update",
                "node": node_name,
                "status": node_state.get("status", "running"),
                "items_found": len(accumulated_state.get("results", [])),
                "elapsed": time.time() - start_time
            }

        # Build final result in same format as run_research()
        elapsed = time.time() - start_time
        final_result = {
            "results": accumulated_state.get("results", []),
            "provenance": [],
            "metrics": {
                "round_number": accumulated_state.get("round_number", 0),
                "queries_used": accumulated_state.get("queries_used", 0),
                "time_elapsed": elapsed,
                "items_found": len(accumulated_state.get("results", []))
            },
            "status": accumulated_state.get("status", "complete"),
            "stop_reason": accumulated_state.get("stop_reason"),
            "errors": accumulated_state.get("errors", []),
            "plan": accumulated_state.get("plan", {})
        }

        yield {
            "event_type": "complete",
            "elapsed": elapsed,
            "final_result": final_result
        }

    except Exception as e:
        elapsed = time.time() - start_time
        error_result = {
            "results": accumulated_state.get("results", []),
            "provenance": [],
            "metrics": {"time_elapsed": elapsed},
            "status": "failed",
            "stop_reason": f"execution_error: {str(e)}",
            "errors": [{"stage": "execution", "error": str(e)}],
            "plan": accumulated_state.get("plan", {})
        }
        yield {
            "event_type": "error",
            "error": str(e),
            "elapsed": elapsed,
            "final_result": error_result
        }
