% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/initialize_agent.R
\name{initialize_agent}
\alias{initialize_agent}
\title{Initialize the ASA Search Agent}
\usage{
initialize_agent(
  backend = "openai",
  model = "gpt-4.1-mini",
  conda_env = "asa_env",
  proxy = "socks5h://127.0.0.1:9050",
  use_memory_folding = TRUE,
  memory_threshold = 4L,
  memory_keep_recent = 2L,
  rate_limit = 0.2,
  timeout = 120L,
  verbose = TRUE
)
}
\arguments{
\item{backend}{LLM backend to use. One of: "openai", "groq", "xai", "exo", "openrouter"}

\item{model}{Model identifier (e.g., "gpt-4.1-mini", "llama-3.3-70b-versatile")}

\item{conda_env}{Name of the conda environment with Python dependencies}

\item{proxy}{SOCKS5 proxy URL for Tor (default: "socks5h://127.0.0.1:9050").
Set to NULL to disable proxy.}

\item{use_memory_folding}{Enable DeepAgent-style memory compression (default: TRUE)}

\item{memory_threshold}{Number of messages before folding triggers (default: 4)}

\item{memory_keep_recent}{Number of recent messages to preserve after folding (default: 2)}

\item{rate_limit}{Requests per second for rate limiting (default: 0.2)}

\item{timeout}{Request timeout in seconds (default: 120)}

\item{verbose}{Print status messages (default: TRUE)}
}
\value{
An object of class \code{asa_agent} containing the initialized agent
  and configuration.
}
\description{
Initializes the Python environment and creates the LangGraph agent with
search tools (Wikipedia, DuckDuckGo). The agent can use multiple LLM
backends and supports DeepAgent-style memory folding.
}
\details{
The agent is created with two tools:
\itemize{
  \item Wikipedia: For looking up encyclopedic information
  \item DuckDuckGo Search: For web searches with a 4-tier fallback system
    (PRIMP -> Selenium -> DDGS library -> raw requests)
}

Memory folding (enabled by default) compresses older messages into a summary
to manage context length in long conversations, following the DeepAgent paper.
}
\section{API Keys}{

The following environment variables should be set based on your backend:
\itemize{
  \item OpenAI: \code{OPENAI_API_KEY}
  \item Groq: \code{GROQ_API_KEY}
  \item xAI: \code{XAI_API_KEY}
  \item OpenRouter: \code{OPENROUTER_API_KEY}
}
}

\section{OpenRouter Models}{

When using the \code{"openrouter"} backend, model names must be in
\code{provider/model-name} format. Examples:
\itemize{
  \item \code{"openai/gpt-4o"}
  \item \code{"anthropic/claude-3-sonnet"}
  \item \code{"google/gemma-2-9b-it:free"}
  \item \code{"meta-llama/llama-3-70b-instruct"}
}
See \url{https://openrouter.ai/models} for available models.
}

\examples{
\dontrun{
# Initialize with OpenAI
agent <- initialize_agent(
  backend = "openai",
  model = "gpt-4.1-mini"
)

# Initialize with Groq and custom settings
agent <- initialize_agent(
  backend = "groq",
  model = "llama-3.3-70b-versatile",
  use_memory_folding = FALSE,
  proxy = NULL  # No Tor proxy
)

# Initialize with OpenRouter (access to 100+ models)
agent <- initialize_agent(
  backend = "openrouter",
  model = "anthropic/claude-3-sonnet"  # Note: provider/model format
)
}

}
\seealso{
\code{\link{run_task}}, \code{\link{run_task_batch}}
}
